{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wc1UngCSQj-q"
   },
   "source": [
    "## Initial Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoEMsFH4Rd6K"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import metrics\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 9)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables which will be necessary later for the Recurrent Neural Network are initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMMv9AucANVA"
   },
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 6400\n",
    "tf.random.set_random_seed(13)\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "STEP = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebTYJhFaSh-G"
   },
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV files are imported into Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "colab_type": "code",
    "id": "3JEL3Jb1Jgwh",
    "outputId": "82162156-2602-4a4c-c582-21b41f221644"
   },
   "outputs": [],
   "source": [
    "tempUrl = 'https://raw.githubusercontent.com/buds-lab/the-building-data-genome-project/master/data/raw/temp_open_utc.csv'\n",
    "tempOpen = pd.read_csv(tempUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTemp = pd.DataFrame()\n",
    "\n",
    "metaUrl = 'https://raw.githubusercontent.com/buds-lab/the-building-data-genome-project/master/data/raw/meta_open.csv'\n",
    "metaOpen = pd.read_csv(metaUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions return the hour or day of the year based on the date inputted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCj-UUwwLZ7n"
   },
   "outputs": [],
   "source": [
    "def getHour(dayx):\n",
    "    return dayx.hour\n",
    "\n",
    "def getDayOfYear(dayy):\n",
    "    return dayy.timetuple().tm_yday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getWeather functions gathers weather data from the repository for the building, cleans it and prepares it to be merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wH7OLjC7J-08"
   },
   "outputs": [],
   "source": [
    "def getWeather(weatherNo):\n",
    "    # Weather columns:\n",
    "    #timestamp\tConditions\tDateUTC<br />\tDew PointC\tEvents\tGust SpeedKm/h\tHumidity\t\n",
    "    #Precipitationmm\tSea Level PressurehPa\tTemperatureC\tTimeBST\tTimeGMT\tVisibilityKm\tWind Direction\tWind SpeedKm/h\tWindDirDegrees\ttimestamp\n",
    "\n",
    "    weatherUrl = \"https://raw.githubusercontent.com/buds-lab/the-building-data-genome-project/master/data/external/weather/\" + weatherNo\n",
    "\n",
    "    weather1x = pd.read_csv(weatherUrl, index_col = 0)\n",
    "    weather1x = weather1x.rename(columns = {'Wind SpeedKm/h' : \"WindSpeedKmh\"})\n",
    "\n",
    "    weather1 = weather1x[['TemperatureC','Humidity','Sea Level PressurehPa', 'Dew PointC', 'VisibilityKm', \"WindSpeedKmh\"]].copy()\n",
    "\n",
    "    county = 0\n",
    "    for y in weather1['WindSpeedKmh']:\n",
    "        if y == 'Calm':\n",
    "            weather1.iloc[county,5] = np.NaN\n",
    "        county += 1\n",
    "\n",
    "    weather1 = weather1.astype(np.float)\n",
    "\n",
    "\n",
    "    weather1.index = pd.to_datetime(weather1.index, utc = True)\n",
    "\n",
    "    # Weather files are resampled to align timestamps with other data\n",
    "    weather1 = weather1.resample('60min').mean()\n",
    "    #weather1.to_csv('weather1resampled.csv')\n",
    "\n",
    "    #Interpolating missing data in the key columns\n",
    "    weather1['TemperatureC'] = weather1['TemperatureC'].interpolate()\n",
    "    weather1['Humidity'] = weather1['Humidity'].interpolate()\n",
    "    weather1['Sea Level PressurehPa'] = weather1['Sea Level PressurehPa'].interpolate()\n",
    "\n",
    "    weather1['Dew PointC'] = weather1['Dew PointC'].interpolate()\n",
    "    weather1['VisibilityKm'] = weather1['VisibilityKm'].interpolate()\n",
    "    weather1['WindSpeedKmh'] = weather1['WindSpeedKmh'].interpolate()\n",
    "\n",
    "\n",
    "\n",
    "    # Removing outliers and replacing with NaNs to be interpolated\n",
    "    z = np.abs(stats.zscore(weather1))\n",
    "\n",
    "    # Outliers, defined as having a zscore gt 5, are replaced with NaNs\n",
    "    x,y = np.where(z > 5)\n",
    "    for f in range(x.size):\n",
    "        weather1.iloc[x[f],y[f]] = np.NaN\n",
    "\n",
    "    # These are again interpolated\n",
    "    weather1['TemperatureC'] = weather1['TemperatureC'].interpolate()\n",
    "    weather1['Humidity'] = weather1['Humidity'].interpolate()\n",
    "    weather1['Sea Level PressurehPa'] = weather1['Sea Level PressurehPa'].interpolate()\n",
    "\n",
    "    weather1['Dew PointC'] = weather1['Dew PointC'].interpolate()\n",
    "    weather1['VisibilityKm'] = weather1['VisibilityKm'].interpolate()\n",
    "    weather1['WindSpeedKmh'] = weather1['WindSpeedKmh'].interpolate()\n",
    "\n",
    "    weather1['dayOfTheWeek'] = weather1.index\n",
    "    weather1['dayOfTheWeek'] = weather1['dayOfTheWeek'].apply(datetime.weekday)\n",
    "\n",
    "    weather1['hourOfTheDay'] = weather1.index\n",
    "    weather1['hourOfTheDay'] = weather1['hourOfTheDay'].apply(getHour)\n",
    "\n",
    "    weather1['dayOfTheYear'] = weather1.index\n",
    "    weather1['dayOfTheYear'] = weather1['dayOfTheYear'].apply(getDayOfYear)\n",
    "\n",
    "    weather1['dayOfTheWeek'] = weather1['dayOfTheWeek'].fillna(method = 'ffill')\n",
    "    weather1['hourOfTheDay'] = weather1['hourOfTheDay'].fillna(method = 'ffill')\n",
    "    weather1['dayOfTheYear'] = weather1['dayOfTheYear'].fillna(method = 'ffill')\n",
    "\n",
    "    return weather1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following creates a dictionary where building names are matched up to a few key pieces of their meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnDict = {}\n",
    "\n",
    "for index, row in metaOpen.iterrows():\n",
    "    columnDict[row['uid']] = [row['sqm'],row['newweatherfilename'],row['primaryspaceusage'],row['timezone']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the files are collated. A counter is used to ensure only 3 buildings are done (for time purposes) but this can be removed to do all buildings. A CSV file containing all relevant building data is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1S48jWuYJWkO"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for column in tempOpen:\n",
    "    # Counter used here to stop at 3 buildings, for testing purposes\n",
    "    if counter == 3:\n",
    "        break\n",
    "\n",
    "    if counter != 0 and column != 'timestamp':\n",
    "        dfTemp['timestamp'] = tempOpen['timestamp']\n",
    "        dfTemp['Usage'] = (tempOpen[column])\n",
    "        \n",
    "        dfTemp['sqm'] = columnDict[column][0]\n",
    "        dfTemp['newweatherfilename'] = columnDict[column][1]\n",
    "        dfTemp['primaryspaceusage'] = columnDict[column][2]\n",
    "        dfTemp['timezone'] = columnDict[column][3]\n",
    "\n",
    "        newWeatherFile = getWeather(columnDict[column][1])\n",
    "\n",
    "        dfTemp['timestamp'] = pd.to_datetime(dfTemp['timestamp'])\n",
    "        mergedTempDf = pd.merge(dfTemp,newWeatherFile, on='timestamp')\n",
    "\n",
    "        mergedTempDf = mergedTempDf.set_index('timestamp')\n",
    "\n",
    "\n",
    "        df_interpol = mergedTempDf\\\n",
    "                .resample('H')\\\n",
    "                .mean()\n",
    "        df_interpol['Usage'] = df_interpol['Usage'].interpolate()\n",
    "        \n",
    "        dfx = df_interpol[df_interpol.isna().any(axis=1)]\n",
    "        \n",
    "        df_interpol.to_csv(column + '_Interpol.csv')\n",
    "\n",
    "        dfTemp.drop(['Usage', \"timestamp\"], axis = 1, inplace = True)\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is then put into a DataFrame and normalized. Some of the relevant features are plotted before and after normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "JPnjXJhZSg0u",
    "outputId": "8699cacd-74d2-45f6-bbf1-bc84f9420d9f"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('PrimClass_Jaylin_Interpol.csv')\n",
    "\n",
    "features_considered = ['Usage', 'TemperatureC', 'Humidity','Sea Level PressurehPa',\n",
    "                       'Dew PointC', 'VisibilityKm', \"WindSpeedKmh\", 'dayOfTheWeek', 'hourOfTheDay']\n",
    "\n",
    "features = df[features_considered]\n",
    "features.index = df['timestamp']\n",
    "\n",
    "dataset1 = np.zeros(shape=(0,0))\n",
    "\n",
    "dataset1 = features.values\n",
    "data_mean = np.nanmean(dataset1[:TRAIN_SPLIT],axis=0)\n",
    "data_std = np.nanstd(dataset1[:TRAIN_SPLIT],axis=0)\n",
    "\n",
    "dataset1df = pd.DataFrame(dataset1, columns = ['Usage', 'TemperatureC', 'Humidity','Sea Level PressurehPa',\n",
    "                       'Dew PointC', 'VisibilityKm', \"WindSpeedKmh\", 'dayOfTheWeek', 'hourOfTheDay'])\n",
    "\n",
    "dataset1df[['TemperatureC','Usage', 'Humidity']].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = (dataset1-data_mean)/data_std\n",
    "\n",
    "dataset1df = pd.DataFrame(dataset1, columns = ['Usage', 'TemperatureC', 'Humidity','Sea Level PressurehPa',\n",
    "                       'Dew PointC', 'VisibilityKm', \"WindSpeedKmh\", 'dayOfTheWeek', 'hourOfTheDay'])\n",
    "\n",
    "usageDataMaster = dataset1df.copy()\n",
    "\n",
    "dataset1df[['TemperatureC','Usage', 'Humidity']].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDIIPfPVANYz"
   },
   "source": [
    "Predictions using all features are quite poor so we now perform feature selection, using a pairwise correlation feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are trying to predict the building metering data (denoted as Usage), we sort the correlations with Usage in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedFeatures = pd.DataFrame((dataset1df.corr()['Usage'].abs().sort_values(ascending = False)))\n",
    "sortedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestFeaturesPearsonCorr = [(dataset1df.corr()['Usage'].abs().sort_values(ascending = False)).index]\n",
    "bestFeaturesPearsonCorr = bestFeaturesPearsonCorr[0]\n",
    "bestFeaturesPearsonCorr = bestFeaturesPearsonCorr[:4]\n",
    "bestFeaturesPearsonCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedFeaturesCrossCorr = pd.DataFrame((dataset1df.rolling(24).corr(pairwise=True).dropna()).corr()['Usage'].abs().sort_values(ascending = False))\n",
    "sortedFeaturesCrossCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestFeaturesCrossCorr = [(dataset1df.rolling(24).corr(pairwise=True).dropna()).corr()['Usage'].abs().sort_values(ascending = False).index]\n",
    "bestFeaturesCrossCorr = bestFeaturesCrossCorr[0]\n",
    "bestFeaturesCrossCorr = bestFeaturesCrossCorr[:4]\n",
    "bestFeaturesCrossCorr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTLiETNdANY9"
   },
   "source": [
    "From this table we can see the features which correlate the most with usage (PrimClass_Jaylin) are Humidity, Wind Speed and the day of the week. Therefore we will make a dataset with only these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rnGYjvNEANay"
   },
   "outputs": [],
   "source": [
    "dates = np.arange(0,1568)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "1A9tX06HANa4",
    "outputId": "aa45e299-35ec-4777-f0d2-2aa464639328"
   },
   "outputs": [],
   "source": [
    "usageData = usageDataMaster.copy()\n",
    "\n",
    "usageData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a new feature 'Usage24Ahead', which is the Usage data for the time 24 hours after that time stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XDMOOvKmANbM"
   },
   "outputs": [],
   "source": [
    "usageData['Usage24Ahead'] = usageData['Usage'].shift(-24)\n",
    "usageData = usageData.dropna()\n",
    "\n",
    "for x in usageData:\n",
    "    if x not in bestFeaturesPearsonCorr and x != 'Usage24Ahead':\n",
    "        usageData.drop([x], axis = 1, inplace = True)\n",
    "\n",
    "#usageData.drop(['TemperatureC','Sea Level PressurehPa','Dew PointC', 'VisibilityKm','hourOfTheDay'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFeUuW8iANar"
   },
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we created the SVM. We must first separate the values into the dependent variables (X_svm) and the independent variable which we are trying to predict (y_svm). We then split this data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EKe7m5qNANbb"
   },
   "outputs": [],
   "source": [
    "X_svm = np.array(usageData[[bestFeaturesPearsonCorr]])\n",
    "#X_svm = np.array(usageData[bestFeaturesCrossCorr])\n",
    "y_svm = np.array(usageData['Usage24Ahead'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6MGbeeyANbf"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X_svm, y_svm, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM is predicting 24 hours ahead using 24 hours of past history, with a step of 1 (one hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHYWg90QANbx"
   },
   "outputs": [],
   "source": [
    "svm_past_history = 24\n",
    "svm_future_target = 24\n",
    "svm_STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "37jAJPskANb-"
   },
   "outputs": [],
   "source": [
    "usageData.drop(['Usage24Ahead'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wtV-8iuVANcL"
   },
   "outputs": [],
   "source": [
    "usageDataArray = np.array(usageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lOzQ75ZSANce"
   },
   "outputs": [],
   "source": [
    "def multivariate_data_no_shift(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "  data = []\n",
    "  labels = []\n",
    "\n",
    "  start_index = start_index + history_size\n",
    "  if end_index is None:\n",
    "    end_index = len(dataset) - target_size\n",
    "\n",
    "  for i in range(start_index, end_index):\n",
    "    indices = range(i-history_size, i, step)\n",
    "    data.append(dataset[indices])\n",
    "\n",
    "    if single_step:\n",
    "      labels.append(target[i+target_size])\n",
    "    else:\n",
    "      labels.append(target[i:i+target_size])\n",
    "\n",
    "  return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is segmented into 24 hour windows to allow us to evaluate and visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKblMdq2ANcl"
   },
   "outputs": [],
   "source": [
    "svm_x_train, svm_y_train = multivariate_data_no_shift(usageDataArray[:,0:4], usageDataArray[:, 0], 0,\n",
    "                                                   TRAIN_SPLIT, svm_past_history,\n",
    "                                                   svm_future_target, svm_STEP)\n",
    "svm_x_val, svm_y_val = multivariate_data_no_shift(usageDataArray[:,0:4], usageDataArray[:, 0],\n",
    "                                               TRAIN_SPLIT, None, svm_past_history,\n",
    "                                               svm_future_target, svm_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "4NfZyjadANcw",
    "outputId": "3bd7b37f-74ed-4e40-e384-8973d8801d15"
   },
   "outputs": [],
   "source": [
    "print ('Single window of past history : {}'.format(svm_x_train[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBhGvDCzANc3"
   },
   "outputs": [],
   "source": [
    "train_data_svm = tf.data.Dataset.from_tensor_slices((svm_x_train, svm_y_train))\n",
    "train_data_svm = train_data_svm.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_svm = tf.data.Dataset.from_tensor_slices((svm_x_val, svm_y_val))\n",
    "val_data_svm = val_data_svm.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM is then fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHiI_4_xANdG"
   },
   "outputs": [],
   "source": [
    "reg_svr = SVR()\n",
    "reg_svr.fit(X_train_svm, y_train_svm)\n",
    "y_pred_svm = reg_svr.predict(svm_x_val[1020])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a visualization of one 24 hour period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "3450l8G3ANdT",
    "outputId": "935674ef-0592-47cf-b486-0a2e05a1bbdd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dates = np.arange(0,24)\n",
    "    \n",
    "plt.plot(dates, (svm_y_val[1020]), c='b', label='Data')\n",
    "plt.plot(dates, y_pred_svm, c='r', label='Linear model')\n",
    "    \n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Support Vector Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make a prediction of the entire dataset and plot part of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKxJVBpvANdY"
   },
   "outputs": [],
   "source": [
    "full_pred = reg_svr.predict(X_test_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "colab_type": "code",
    "id": "hA8fAOtFANd0",
    "outputId": "4b0d93e5-a632-4888-e94c-ae023ba8c427"
   },
   "outputs": [],
   "source": [
    "dates = np.arange(0,1748)\n",
    "  \n",
    "font = {'family': 'DejaVu Sans',\n",
    "        'color':  'black',\n",
    "        'weight': 'normal',\n",
    "        'size': 25,\n",
    "        }\n",
    "\n",
    "plt.plot(dates[:120], y_test_svm[:120], c='b', label='Data')\n",
    "plt.plot(dates[:120], full_pred[:120], c='r', label='Linear model')\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlabel('Hours',fontdict=font)\n",
    "plt.ylabel('Usage (Normalized)',fontdict=font)\n",
    "plt.title('Support Vector Regression',fontdict=font)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make a batch of predictions which will be used for evaluation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AX9byFQmWBsW"
   },
   "outputs": [],
   "source": [
    "svm_comp_pred = []\n",
    "\n",
    "for z in range(256):\n",
    "    svm_comp_pred.append(reg_svr.predict(svm_x_val[z]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYGTj7p77AqO"
   },
   "source": [
    "# RNN - 24 Hour Training Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define some functions necessary for the RNN. multi_step_plot allows us to visualize the multi hour predictions. create_time_steps is used for plotting visualizations. plot_train_history is used to plot the error over the course of training a model. Multivariate_data is a method of segmenting the data for use in the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ck6Nm85RTenW"
   },
   "outputs": [],
   "source": [
    "def multi_step_plot(history, true_future, prediction):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_in = create_time_steps(len(history))\n",
    "    num_out = len(true_future)\n",
    "\n",
    "    plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
    "    plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'b', label='True Future')\n",
    "    \n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'r', label='Predicted Future')\n",
    "    \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_LU6K7ITl3R"
   },
   "outputs": [],
   "source": [
    "def create_time_steps(length):\n",
    "  return list(range(-length, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FmiA8dC4VVCj"
   },
   "outputs": [],
   "source": [
    "def plot_train_history(history, title, mse = False, rmse = False):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    if mse:\n",
    "        mse = history.history['mean_squared_error']\n",
    "    if rmse:\n",
    "        rmse = history.history['root_mean_squared_error']\n",
    "        val_rmse = history.history['val_root_mean_squared_error']\n",
    "    \n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    if mse:\n",
    "        plt.plot(epochs, mse, 'y', label='Mean Squared Error')\n",
    "    if rmse:\n",
    "        plt.plot(epochs, rmse, 'g', label='Root Mean Squared Error')\n",
    "        plt.plot(epochs, val_rmse, 'm', label='Validation Root Mean Squared Error')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wf9l7J1tTTr9"
   },
   "outputs": [],
   "source": [
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "  data = []\n",
    "  labels = []\n",
    "\n",
    "  start_index = start_index + history_size\n",
    "  if end_index is None:\n",
    "    end_index = len(dataset) - target_size\n",
    "\n",
    "  for i in range(start_index, end_index):\n",
    "    indices = range(i-history_size, i, step)\n",
    "    data.append(dataset[indices])\n",
    "\n",
    "    if single_step:\n",
    "      labels.append(target[i+target_size])\n",
    "    else:\n",
    "      labels.append(target[i:i+target_size])\n",
    "\n",
    "  return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZfgyyhgT-IP"
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we segment the data, layout the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7z810VXsANf0"
   },
   "outputs": [],
   "source": [
    "mvms_past_history = 24\n",
    "mvms_future_target = 24\n",
    "mvms_STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tiA8uZN9ANgI"
   },
   "outputs": [],
   "source": [
    "x_train_multi, y_train_multi = multivariate_data(usageDataArray, usageDataArray[:, 0], 0,\n",
    "                                                 TRAIN_SPLIT, mvms_past_history,\n",
    "                                                 mvms_future_target, mvms_STEP)\n",
    "x_val_multi, y_val_multi = multivariate_data(usageDataArray, usageDataArray[:, 0],\n",
    "                                             TRAIN_SPLIT, None, mvms_past_history,\n",
    "                                             mvms_future_target, mvms_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "TXxijo6-ANgn",
    "outputId": "15830ebe-b4da-4aa7-8aec-c1db86498b2d"
   },
   "outputs": [],
   "source": [
    "print ('Single window of past history : {}'.format(x_train_multi[0].shape))\n",
    "print ('\\n Target usage to predict : {}'.format(y_train_multi[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQH0TkchANgt"
   },
   "outputs": [],
   "source": [
    "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O530IKQe__2O"
   },
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njBg_OhOANhE"
   },
   "outputs": [],
   "source": [
    "multi_step_model = tf.keras.models.Sequential()\n",
    "\n",
    "multi_step_model.add(tf.keras.layers.Conv1D(filters=10, kernel_size=3, activation='relu', input_shape=x_train_multi.shape[-2:]))\n",
    "multi_step_model.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "multi_step_model.add(tf.keras.layers.Dropout(rate=0.1))\n",
    "\n",
    "multi_step_model.add(tf.keras.layers.Conv1D(filters=20, kernel_size=3, activation='relu', input_shape=x_train_multi.shape[-2:]))\n",
    "multi_step_model.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "multi_step_model.add(tf.keras.layers.Dropout(rate=0.1))\n",
    "\n",
    "multi_step_model.add(tf.keras.layers.LSTM(200, activation='relu',return_sequences=True))\n",
    "multi_step_model.add(tf.keras.layers.LSTM(25, activation='relu'))\n",
    "\n",
    "multi_step_model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "\n",
    "multi_step_model.add(tf.keras.layers.Dense(24))\n",
    "\n",
    "multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae', metrics=['mean_squared_error',\n",
    "                                                                                                    root_mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "P1BVCWtbANhH",
    "outputId": "f0b19def-b2ca-47ef-cccc-b1c579f26940",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x, y in val_data_multi.take(1):\n",
    "  print (multi_step_model.predict(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "XlvhVkFfANhL",
    "outputId": "dda56d7a-9ce3-4010-8e05-1544ebd2a553"
   },
   "outputs": [],
   "source": [
    "multi_step_history = multi_step_model.fit(train_data_multi, epochs=10,\n",
    "                                          steps_per_epoch=200,\n",
    "                                          validation_data=val_data_multi,\n",
    "                                          validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "PEhG5XLqANhQ",
    "outputId": "0ae956de-cae6-4fcf-8a06-aaa5a6eb48f5"
   },
   "outputs": [],
   "source": [
    "plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a batch of predictions for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsrfqm46ANhZ"
   },
   "outputs": [],
   "source": [
    "for x, y in val_data_multi.take(1):\n",
    "    rnn_data = []\n",
    "    rnn_comp_pred = []\n",
    "    for a in range(len(x)):\n",
    "        rnn_data.append(y[a])\n",
    "        rnn_comp_pred.append(multi_step_model.predict(x)[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same RNN training and predictions process is then repeated for 48 hour and 72 hours of past history training windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPdCIekoANio"
   },
   "source": [
    "# RNN - 48 Hour Training Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hsuw9j0bANip"
   },
   "outputs": [],
   "source": [
    "long_mvms_past_history = 48\n",
    "mvms_future_target = 24\n",
    "mvms_STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oLL4cxwFANi3"
   },
   "outputs": [],
   "source": [
    "long_x_train_multi, long_y_train_multi = multivariate_data(usageDataArray, usageDataArray[:, 0], 0,\n",
    "                                                 TRAIN_SPLIT, long_mvms_past_history,\n",
    "                                                 mvms_future_target, mvms_STEP)\n",
    "long_x_val_multi, long_y_val_multi = multivariate_data(usageDataArray, usageDataArray[:, 0],\n",
    "                                             TRAIN_SPLIT, None, long_mvms_past_history,\n",
    "                                             mvms_future_target, mvms_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "kUAXYdm0ANjL",
    "outputId": "140a5bd8-9fca-4bf4-80be-17b3c5702f9a"
   },
   "outputs": [],
   "source": [
    "print ('Single window of past history : {}'.format(long_x_train_multi[0].shape))\n",
    "print ('\\n Target usage to predict : {}'.format(long_y_train_multi[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWv-Jqn9ANjT"
   },
   "outputs": [],
   "source": [
    "long_train_data_multi = tf.data.Dataset.from_tensor_slices((long_x_train_multi, long_y_train_multi))\n",
    "long_train_data_multi = long_train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "long_val_data_multi = tf.data.Dataset.from_tensor_slices((long_x_val_multi, long_y_val_multi))\n",
    "long_val_data_multi = long_val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "RA5J-ZEDANjX",
    "outputId": "1dade74f-d101-4bb6-abd6-105053023be2"
   },
   "outputs": [],
   "source": [
    "for x, y in long_train_data_multi.take(1):\n",
    "    multi_step_plot(x[0], y[0], np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLGM37XrANjc"
   },
   "outputs": [],
   "source": [
    "long_multi_step_model = tf.keras.models.Sequential()\n",
    "\n",
    "long_multi_step_model.add(tf.keras.layers.Conv1D(filters=10, kernel_size=3, activation='relu', input_shape=long_x_train_multi.shape[-2:]))\n",
    "long_multi_step_model.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "long_multi_step_model.add(tf.keras.layers.Dropout(rate=0.1))\n",
    "\n",
    "long_multi_step_model.add(tf.keras.layers.Conv1D(filters=20, kernel_size=3, activation='relu', input_shape=long_x_train_multi.shape[-2:]))\n",
    "long_multi_step_model.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "long_multi_step_model.add(tf.keras.layers.Dropout(rate=0.1))\n",
    "\n",
    "long_multi_step_model.add(tf.keras.layers.LSTM(100, activation='relu',return_sequences=True))\n",
    "long_multi_step_model.add(tf.keras.layers.LSTM(100, activation='relu'))\n",
    "\n",
    "long_multi_step_model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "\n",
    "long_multi_step_model.add(tf.keras.layers.Dense(24))\n",
    "\n",
    "\n",
    "long_multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae', metrics=['mean_squared_error',\n",
    "                                                                                                    root_mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "ll4rzC-FANjg",
    "outputId": "39afe288-432b-49e6-87c7-5013d7512a0c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x, y in long_val_data_multi.take(1):\n",
    "  print (long_multi_step_model.predict(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "QUWIHXaOANjl",
    "outputId": "736efeb5-d4a5-404b-a5fe-e9a87620f0cf"
   },
   "outputs": [],
   "source": [
    "long_multi_step_history = long_multi_step_model.fit(long_train_data_multi, epochs=10,\n",
    "                                          steps_per_epoch=200,\n",
    "                                          validation_data=long_val_data_multi,\n",
    "                                          validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "cwEQycVbANjp",
    "outputId": "44d7a1e6-36ea-419d-dddf-f3b2bd835d5d"
   },
   "outputs": [],
   "source": [
    "plot_train_history(long_multi_step_history, 'Multi-Step Training and validation loss', rmse= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KYbGfhLbANj5"
   },
   "outputs": [],
   "source": [
    "long_rnn_data = []\n",
    "long_rnn_comp_pred = []\n",
    "\n",
    "for x, y in long_val_data_multi.take(1):\n",
    "    for a in range(len(x)):\n",
    "        long_rnn_data.append(y[a])\n",
    "        long_rnn_comp_pred.append(long_multi_step_model.predict(x)[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "ar2rXZ7-ANkC",
    "outputId": "959ed255-d0e2-44f8-f0fc-51b0c2da27c3"
   },
   "outputs": [],
   "source": [
    "dates = np.arange(0,24)\n",
    "    \n",
    "plt.plot(dates, long_rnn_data[200], c='b', label='Data')\n",
    "plt.plot(dates, long_rnn_comp_pred[200], c= 'y', label='RNN model')\n",
    "    \n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Prediction Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8FVEharjmXK"
   },
   "source": [
    "# RNN - 72 Hour Training Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "to1bthW7jmXQ"
   },
   "outputs": [],
   "source": [
    "three_day_mvms_past_history = 72\n",
    "mvms_future_target = 24\n",
    "mvms_STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFe1uiK4jmXU"
   },
   "outputs": [],
   "source": [
    "three_day_x_train_multi, three_day_y_train_multi = multivariate_data(usageDataArray, usageDataArray[:, 0], 0,\n",
    "                                                 TRAIN_SPLIT, three_day_mvms_past_history,\n",
    "                                                 mvms_future_target, mvms_STEP)\n",
    "three_day_x_val_multi, three_day_y_val_multi = multivariate_data(usageDataArray, usageDataArray[:, 0],\n",
    "                                             TRAIN_SPLIT, None, three_day_mvms_past_history,\n",
    "                                             mvms_future_target, mvms_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "21NSVQWCjmXX",
    "outputId": "ed32f9b9-d0da-4fd1-906c-42ff844f7192"
   },
   "outputs": [],
   "source": [
    "print ('Single window of past history : {}'.format(three_day_x_train_multi[0].shape))\n",
    "print ('\\n Target usage to predict : {}'.format(three_day_y_train_multi[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pzbC61MsjmXg"
   },
   "outputs": [],
   "source": [
    "three_day_train_data_multi = tf.data.Dataset.from_tensor_slices((three_day_x_train_multi, three_day_y_train_multi))\n",
    "three_day_train_data_multi = three_day_train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "three_day_val_data_multi = tf.data.Dataset.from_tensor_slices((three_day_x_val_multi, three_day_y_val_multi))\n",
    "three_day_val_data_multi = three_day_val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "zCW9IxNdjmXi",
    "outputId": "6b9b87a1-f141-481c-b766-50556415dcee"
   },
   "outputs": [],
   "source": [
    "for x, y in three_day_train_data_multi.take(1):\n",
    "    multi_step_plot(x[0], y[0], np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OGibDgbjmXl"
   },
   "outputs": [],
   "source": [
    "three_day_multi_step_model = tf.keras.models.Sequential()\n",
    "\n",
    "three_day_multi_step_model.add(tf.keras.layers.Conv1D(filters=10, kernel_size=3, activation='relu', input_shape=three_day_x_train_multi.shape[-2:]))\n",
    "three_day_multi_step_model.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "three_day_multi_step_model.add(tf.keras.layers.Dropout(rate=0.1))\n",
    "\n",
    "three_day_multi_step_model.add(tf.keras.layers.Conv1D(filters=20, kernel_size=3, activation='relu', input_shape=three_day_x_train_multi.shape[-2:]))\n",
    "three_day_multi_step_model.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "three_day_multi_step_model.add(tf.keras.layers.Dropout(rate=0.1))\n",
    "\n",
    "three_day_multi_step_model.add(tf.keras.layers.LSTM(100, activation='relu',return_sequences=True))\n",
    "three_day_multi_step_model.add(tf.keras.layers.LSTM(100, activation='relu'))\n",
    "\n",
    "three_day_multi_step_model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "\n",
    "three_day_multi_step_model.add(tf.keras.layers.Dense(24))\n",
    "\n",
    "\n",
    "three_day_multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae', metrics=['mean_squared_error',\n",
    "                                                                                                    root_mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "pg3E1sm8jmXn",
    "outputId": "d4c753c8-b827-457b-902b-124e54629a8b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x, y in three_day_val_data_multi.take(1):\n",
    "  print (three_day_multi_step_model.predict(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "8Brk7eHgjmXp",
    "outputId": "a697f943-1604-41c6-87de-d23db75e766c"
   },
   "outputs": [],
   "source": [
    "three_day_multi_step_history = three_day_multi_step_model.fit(three_day_train_data_multi, epochs=10,\n",
    "                                          steps_per_epoch=200,\n",
    "                                          validation_data=three_day_val_data_multi,\n",
    "                                          validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "B8W2Lri5jmXu",
    "outputId": "ce84dfaf-1832-4eeb-adda-939595bfb825"
   },
   "outputs": [],
   "source": [
    "plot_train_history(three_day_multi_step_history, 'Multi-Step Training and validation loss', rmse= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8C39lecjmXz"
   },
   "outputs": [],
   "source": [
    "three_day_rnn_data = []\n",
    "three_day_rnn_comp_pred = []\n",
    "\n",
    "for x, y in three_day_val_data_multi.take(1):\n",
    "    for a in range(len(x)):\n",
    "        three_day_rnn_data.append(y[a])\n",
    "        three_day_rnn_comp_pred.append(three_day_multi_step_model.predict(x)[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "exS4cCE5jmX1",
    "outputId": "d36b2697-edd9-477f-c510-ecde528a6356"
   },
   "outputs": [],
   "source": [
    "dates = np.arange(0,24)\n",
    "    \n",
    "plt.plot(dates, three_day_rnn_data[200], c='b', label='Data')\n",
    "plt.plot(dates, three_day_rnn_comp_pred[200], c= 'y', label='RNN model')\n",
    "    \n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Prediction Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uesjYbzh_ddg"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions take in 24 hour periods of data and evaluate predictions made over them for three different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HRZWEP-GANh6"
   },
   "outputs": [],
   "source": [
    "def twentyfour_hour_mae(y_pred, y_true):\n",
    "    mae = []\n",
    "    for a in range(len(y_pred)):\n",
    "        mae.append(mean_absolute_error(y_pred[a],y_true[a]))\n",
    "    \n",
    "    return (np.array(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__TlWgtpANiA"
   },
   "outputs": [],
   "source": [
    "def twentyfour_hour_rmse(y_pred, y_true):\n",
    "    rmse = []\n",
    "    for a in range(len(y_pred)):\n",
    "        rmse.append(simple_rmse(y_pred[a],y_true[a]))\n",
    "    \n",
    "    return (np.array(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ov6B_z0Pa2OD"
   },
   "outputs": [],
   "source": [
    "def twentyfour_hour_r_squared(y_pred, y_true):\n",
    "    r_squared = []\n",
    "    for a in range(len(y_pred)):\n",
    "        r_squared.append(get_r_squared(y_pred[a],y_true[a]))\n",
    "    \n",
    "    return (np.array(r_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(actual, forecasted):\n",
    "    return 1/len(actual) * np.sum(2 * np.abs(forecasted - actual) / (np.abs(actual) + np.abs(forecasted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twentyfour_hour_smape(y_pred, y_true):\n",
    "    smape_arr = []\n",
    "    for a in range(len(y_pred)):\n",
    "        smape_arr.append(smape(y_pred[a],y_true[a]))\n",
    "    \n",
    "    return (np.array(smape_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QU-8rUeThFmm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "def simple_rmse(y_true, y_pred):\n",
    "    return sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the error of each model across their batches of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbCyrm6MANkG"
   },
   "outputs": [],
   "source": [
    "rnn_batch_mae = (twentyfour_hour_mae(rnn_comp_pred,long_rnn_data))\n",
    "rnn_batch_rmse = (twentyfour_hour_rmse(rnn_comp_pred,long_rnn_data))\n",
    "rnn_batch_smape = (twentyfour_hour_smape(long_rnn_data,rnn_comp_pred))\n",
    "\n",
    "two_day_rnn_batch_mae = (twentyfour_hour_mae(long_rnn_comp_pred,long_rnn_data))\n",
    "two_day_rnn_batch_rmse = (twentyfour_hour_rmse(long_rnn_comp_pred,long_rnn_data))\n",
    "two_day_rnn_batch_smape = (twentyfour_hour_smape(long_rnn_data,long_rnn_comp_pred))\n",
    "\n",
    "three_day_rnn_batch_mae = (twentyfour_hour_mae(three_day_rnn_comp_pred,long_rnn_data))\n",
    "three_day_rnn_batch_rmse = (twentyfour_hour_rmse(three_day_rnn_comp_pred,long_rnn_data))\n",
    "three_day_rnn_batch_smape = (twentyfour_hour_smape(three_day_rnn_data,long_rnn_comp_pred))\n",
    "\n",
    "svm_batch_mae = (twentyfour_hour_mae(svm_comp_pred,long_rnn_data))\n",
    "svm_batch_rmse = (twentyfour_hour_rmse(svm_comp_pred,long_rnn_data))\n",
    "svm_batch_smape = (twentyfour_hour_smape(long_rnn_data,svm_comp_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the cumulative error for models across their prediction batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "4F11K5uwANkP",
    "outputId": "18c42b89-c07c-4bd5-f619-24e1c921f239"
   },
   "outputs": [],
   "source": [
    "dates = np.arange(0,256)\n",
    "\n",
    "plt.plot(dates, rnn_batch_mae.cumsum(), color = 'b', label='RNN Error')\n",
    "plt.plot(dates, svm_batch_mae.cumsum(), color = 'g', label='SVM Error')\n",
    "\n",
    "plt.xlabel('24 Hour Periods')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Error Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = np.arange(0,256)\n",
    "\n",
    "plt.plot(dates, rnn_batch_rmse.cumsum(), color = 'b', label='RNN Error')\n",
    "plt.plot(dates, svm_batch_rmse.cumsum(), color = 'g', label='SVM Error')\n",
    "\n",
    "plt.xlabel('24 Hour Periods')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Error Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then collate and graph the results of the error across each different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "colab_type": "code",
    "id": "ygvwGxEOYpBu",
    "outputId": "aad0396c-33ff-46a2-c12a-21bfa40a5181"
   },
   "outputs": [],
   "source": [
    "svmBars = [np.mean(svm_batch_mae), np.mean(svm_batch_rmse)]\n",
    "rnnBars = [np.mean(two_day_rnn_batch_mae), np.mean(two_day_rnn_batch_rmse)]\n",
    "\n",
    "groupedBarErrorData = np.array([[\"SVM\", \"MAE\", np.mean(svm_batch_mae)],\n",
    "                                [\"SVM\",\"RMSE\", np.mean(svm_batch_rmse)], \n",
    "                                [\"SVM\", \"SMAPE\", np.mean(svm_batch_smape)], \n",
    "                                [\"24H-RNN\", \"MAE\", np.mean(rnn_batch_mae)],\n",
    "                                [\"24H-RNN\", \"RMSE\", np.mean(rnn_batch_rmse)],\n",
    "                                [\"24H-RNN\", \"SMAPE\", np.mean(rnn_batch_smape)]\n",
    "                               ])\n",
    "\n",
    "rnnGroupedBarErrorData = np.array([[\"24H-RNN\", \"MAE\", np.mean(rnn_batch_mae)],\n",
    "                                [\"24H-RNN\", \"RMSE\", np.mean(rnn_batch_rmse)],\n",
    "                                [\"24H-RNN\", \"SMAPE\", np.mean(rnn_batch_smape)],\n",
    "                                [\"48H-RNN\", \"MAE\", np.mean(two_day_rnn_batch_mae)],\n",
    "                                [\"48H-RNN\", \"RMSE\", np.mean(two_day_rnn_batch_rmse)],\n",
    "                                [\"48H-RNN\", \"SMAPE\", np.mean(two_day_rnn_batch_smape)],\n",
    "                                [\"72H-RNN\", \"MAE\", np.mean(three_day_rnn_batch_mae)],\n",
    "                                [\"72H-RNN\", \"RMSE\", np.mean(three_day_rnn_batch_rmse)],\n",
    "                                [\"72H-RNN\", \"SMAPE\", np.mean(three_day_rnn_batch_smape)]])\n",
    "\n",
    "groupedBarErrorDataFrame = pd.DataFrame(groupedBarErrorData, columns=[\"Model\", \"Metric\", \"Accuracy\"])\n",
    "rnnGroupedBarErrorDataFrame = pd.DataFrame(rnnGroupedBarErrorData, columns=[\"Model\", \"Metric\", \"Accuracy\"])\n",
    "\n",
    "groupedBarErrorDataFrame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "EA0xvHjkas6a",
    "outputId": "be6bceb0-5331-410a-c5fb-ebefa84ab827"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(x=\"Metric\", y=\"Accuracy\", hue=\"Model\", data=groupedBarErrorDataFrame,\n",
    "                height=6, kind=\"bar\", palette=\"bright\")\n",
    "g.despine(left=True)\n",
    "g.set_ylabels(\"Error\\n\", fontsize = 16)\n",
    "g.set_xlabels(\"\\nMetric\", fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "SGkD4oQwpCfs",
    "outputId": "a3551364-681f-4e6c-b527-0e5e6852d6c7"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(x=\"Metric\", y=\"Accuracy\", hue=\"Model\", data=rnnGroupedBarErrorDataFrame,\n",
    "                height=6, kind=\"bar\", palette=\"muted\")\n",
    "g.despine(left=True)\n",
    "g.set_ylabels(\"Error\\n\", fontsize = 16)\n",
    "g.set_xlabels(\"\\nMetric\", fontsize = 16)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wc1UngCSQj-q",
    "ebTYJhFaSh-G",
    "qFeUuW8iANar",
    "UYGTj7p77AqO",
    "RPdCIekoANio",
    "U8FVEharjmXK"
   ],
   "name": "Clean Layout.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
