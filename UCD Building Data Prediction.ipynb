{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "import os.path\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 9)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable initalizations to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_random_seed(13)\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "STEP = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file containing the airport weather data is read in, skipping rows which contain unneeded information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dubAirUrl = 'https://raw.githubusercontent.com/MatthewBarrettUCD/Final-Year-Project/master/hly532.csv'\n",
    "dubAirWD = pd.read_csv(dubAirUrl, skiprows = 22)\n",
    "dubAirWD.columns = dubAirWD.iloc[0]\n",
    "dubAirWD = dubAirWD.drop(dubAirWD.index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV containing the UCD Veterinary Building Data is then read in. The data is prepared into a usable format for the models.\n",
    "\n",
    "NOTE: This data is available upon request, placeholder data will be generated if it is not present. This will affect the accuracy of the models as no correlation will exist with the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder = False\n",
    "\n",
    "if os.path.isfile('UCD_Belfield_Veterinary Science.csv'):\n",
    "    ucdBD = pd.read_csv('UCD_Belfield_Veterinary Science.csv', skiprows = 1)\n",
    "    ucdBD.drop(['Values'], axis = 1, inplace = True)\n",
    "\n",
    "    ucdBD = ucdBD.stack().reset_index()\n",
    "    ucdBD.drop(['level_0'], axis = 1, inplace = True)\n",
    "else:\n",
    "    placeholderUrl = 'https://raw.githubusercontent.com/MatthewBarrettUCD/Final-Year-Project/master/placeholder_data.csv'\n",
    "    ucdBD = pd.read_csv(placeholderUrl)\n",
    "    ucdBD['Usage'] = np.random.randint(30,150, size=len(ucdBD))\n",
    "    placeholder = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not placeholder:\n",
    "    dateTemp = 0\n",
    "    countBool = False\n",
    "    x = 0\n",
    "\n",
    "    while x < (len(ucdBD)):\n",
    "        if countBool:\n",
    "            countBool = False\n",
    "            x -= 1\n",
    "\n",
    "        if ucdBD.iloc[x,0] == \"Date\":\n",
    "            ucdBD.iloc[x,0] = ucdBD.iloc[x,1]\n",
    "            dateTemp = ucdBD.iloc[x,1]\n",
    "            ucdBD = ucdBD.drop(ucdBD.index[x])\n",
    "            countBool = True\n",
    "        else:\n",
    "            ucdBD.iloc[x,0] = str(dateTemp) + \" \" + ucdBD.iloc[x,0]\n",
    "\n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not placeholder:\n",
    "    ucdBD.columns = ['Date', 'Usage']\n",
    "\n",
    "ucdBD.index = ucdBD['Date']\n",
    "ucdBD.drop(['Date'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not placeholder:\n",
    "    ucdBD.index = pd.to_datetime(ucdBD.index, utc = True)\n",
    "    ucdBD = ucdBD.astype(float).resample('60min').mean()\n",
    "\n",
    "ucdBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ind\" in dubAirWD.columns:\n",
    "    dubAirWD.drop(['ind'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dubAirWD.index = dubAirWD['date']\n",
    "dubAirWD.drop(['date'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dubAirWD.index = pd.to_datetime(dubAirWD.index, utc = True)\n",
    "dubAirWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uawdf = pd.merge(ucdBD,dubAirWD, left_index = True, right_index = True)\n",
    "\n",
    "for z in uawdf.columns:\n",
    "    uawdf[z] = pd.to_numeric(uawdf[z], errors = 'coerce')\n",
    "    \n",
    "for z in uawdf.columns:\n",
    "    if uawdf[z].isna().any() == True:\n",
    "        uawdf[z] = uawdf[z].interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be split with a ratio of 80:20 into training and test data. Additional temporal features such as dayOfTheWeek and hourOfTheDay are added for each hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = int(len(uawdf)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHour(dayx):\n",
    "    return dayx.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uawdf['dayOfTheWeek'] = uawdf.index\n",
    "uawdf['dayOfTheWeek'] = uawdf['dayOfTheWeek'].apply(datetime.weekday)\n",
    "\n",
    "uawdf['hourOfTheDay'] = uawdf.index\n",
    "uawdf['hourOfTheDay'] = uawdf['hourOfTheDay'].apply(getHour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uawdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then normalize the dataset and use a pairwise correlation to find which features have the greatest correlation with the independent variable to be predicted, the building energy usage (Usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uawdf_features_considered = ['Usage', 'rain', 'temp', 'wetb', 'dewpt', 'vappr', 'rhum', 'msl','wdsp', 'wddir', 'ww', 'w', 'sun', 'vis', 'clht', 'clamt','dayOfTheWeek', 'hourOfTheDay']\n",
    "\n",
    "uawdf_features = uawdf[uawdf_features_considered]\n",
    "uawdf_features.index = uawdf.index\n",
    "\n",
    "uawdf_dataset = np.zeros(shape=(0,0))\n",
    "\n",
    "uawdf_dataset = uawdf_features.values\n",
    "uawdf_data_mean = np.nanmean(uawdf_dataset[:TRAIN_SPLIT],axis=0)\n",
    "uawdf_data_std = np.nanstd(uawdf_dataset[:TRAIN_SPLIT],axis=0)\n",
    "\n",
    "uawdf_dataset = (uawdf_dataset-uawdf_data_mean)/uawdf_data_std\n",
    "\n",
    "uawdf_datasetdf = pd.DataFrame(uawdf_dataset, columns = ['Usage', 'rain', 'temp', 'wetb', 'dewpt', 'vappr', 'rhum', 'msl',\n",
    "       'wdsp', 'wddir', 'ww', 'w', 'sun', 'vis', 'clht', 'clamt',\n",
    "       'dayOfTheWeek', 'hourOfTheDay'])\n",
    "\n",
    "ucdUsageDataMaster = uawdf_datasetdf.copy()\n",
    "\n",
    "uawdf_datasetdf.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uawdf_datasetdf.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedFeatures = uawdf_datasetdf.corr()['Usage'].abs().sort_values(ascending = False)\n",
    "sortedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = np.arange(0,1568)\n",
    "\n",
    "ucdUsageData = ucdUsageDataMaster.copy()\n",
    "\n",
    "ucdUsageData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucdUsageData['Usage24Ahead'] = ucdUsageData['Usage'].shift(-24)\n",
    "ucdUsageData = ucdUsageData.dropna()\n",
    "bestFeatures = []\n",
    "\n",
    "for x in range(len(sortedFeatures)):\n",
    "    if x > 5:\n",
    "        ucdUsageData.drop([sortedFeatures.index[x]], axis = 1, inplace = True)\n",
    "    else:\n",
    "        bestFeatures.append(sortedFeatures.index[x])\n",
    "        \n",
    "ucdUsageData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorted by highest correlation with Usage, the best features are found to be sun, rhum (humidity), dayOfTheWeek, wdsp (windspeed) and temp (temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the SVM using these features as the dependent variables and the Usage 24 hours ahead as the independent variable to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucd_X_svm = np.array(ucdUsageData[bestFeatures])\n",
    "ucd_y_svm = np.array(ucdUsageData['Usage24Ahead'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ucd_X_train_svm, ucd_X_test_svm, ucd_y_train_svm, ucd_y_test_svm = train_test_split(ucd_X_svm, ucd_y_svm, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_past_history = 24\n",
    "svm_future_target = 24\n",
    "svm_STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucdUsageData.drop(['Usage24Ahead'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucdUsageDataArray = np.array(ucdUsageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_data_no_shift(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "  data = []\n",
    "  labels = []\n",
    "\n",
    "  start_index = start_index + history_size\n",
    "  if end_index is None:\n",
    "    end_index = len(dataset) - target_size\n",
    "\n",
    "  for i in range(start_index, end_index):\n",
    "    indices = range(i-history_size, i, step)\n",
    "    data.append(dataset[indices])\n",
    "\n",
    "    if single_step:\n",
    "      labels.append(target[i+target_size])\n",
    "    else:\n",
    "      labels.append(target[i:i+target_size])\n",
    "\n",
    "  return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucd_svm_x_train, ucd_svm_y_train = multivariate_data_no_shift(ucdUsageDataArray[:,0:6], ucdUsageDataArray[:, 0], 0,\n",
    "                                                   TRAIN_SPLIT, svm_past_history,\n",
    "                                                   svm_future_target, svm_STEP)\n",
    "ucd_svm_x_val, ucd_svm_y_val = multivariate_data_no_shift(ucdUsageDataArray[:,0:6], ucdUsageDataArray[:, 0],\n",
    "                                               TRAIN_SPLIT, None, svm_past_history,\n",
    "                                               svm_future_target, svm_STEP)\n",
    "\n",
    "print ('Single window of past history : {}'.format(ucd_svm_x_train[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucd_train_data_svm = tf.data.Dataset.from_tensor_slices((ucd_svm_x_train, ucd_svm_y_train))\n",
    "ucd_train_data_svm = ucd_train_data_svm.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "ucd_val_data_svm = tf.data.Dataset.from_tensor_slices((ucd_svm_x_val, ucd_svm_y_val))\n",
    "ucd_val_data_svm = ucd_val_data_svm.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the SVR and then plot one 24 hour period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucd_reg_svr = SVR()\n",
    "ucd_reg_svr.fit(ucd_X_train_svm, ucd_y_train_svm)\n",
    "ucd_y_pred_svm = ucd_reg_svr.predict(ucd_svm_x_val[1020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = np.arange(0,24)\n",
    "    \n",
    "plt.plot(dates, (ucd_svm_y_val[1020]), c='b', label='Data')\n",
    "plt.plot(dates, ucd_y_pred_svm, c='r', label='Linear model')\n",
    "    \n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Support Vector Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucd_full_pred = ucd_reg_svr.predict(ucd_X_test_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot 100 hours of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = np.arange(0,100)\n",
    "  \n",
    "font = {'family': 'DejaVu Sans',\n",
    "        'color':  'black',\n",
    "        'weight': 'normal',\n",
    "        'size': 25,\n",
    "        }\n",
    "\n",
    "plt.plot(dates, ucd_y_test_svm[:100], c='b', label='Data')\n",
    "plt.plot(dates, ucd_full_pred[:100], c='r', label='Linear model')\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlabel('Hours',fontdict=font)\n",
    "plt.ylabel('Usage (Normalized)',fontdict=font)\n",
    "plt.title('Support Vector Regression',fontdict=font)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make a batch of 2560 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucd_svm_comp_pred = []\n",
    "\n",
    "for z in range(2560):\n",
    "    ucd_svm_comp_pred.append(ucd_reg_svr.predict(ucd_svm_x_val[z]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN - 24 Hour Training Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_plot(history, true_future, prediction):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_in = create_time_steps(len(history))\n",
    "    num_out = len(true_future)\n",
    "\n",
    "    plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
    "    plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'b', label='True Future')\n",
    "    \n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'r', label='Predicted Future')\n",
    "    \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def create_time_steps(length):\n",
    "  return list(range(-length, 0))\n",
    "\n",
    "def plot_train_history(history, title, rmse = False):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    mse = history.history['mean_squared_error']\n",
    "    if rmse:\n",
    "        rmse = history.history['root_mean_squared_error']\n",
    "        val_rmse = history.history['val_root_mean_squared_error']\n",
    "    \n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.plot(epochs, mse, 'y', label='Mean Squared Error')\n",
    "    if rmse:\n",
    "        plt.plot(epochs, rmse, 'g', label='Root Mean Squared Error')\n",
    "        plt.plot(epochs, val_rmse, 'm', label='Validation Root Mean Squared Error')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "  data = []\n",
    "  labels = []\n",
    "\n",
    "  start_index = start_index + history_size\n",
    "  if end_index is None:\n",
    "    end_index = len(dataset) - target_size\n",
    "\n",
    "  for i in range(start_index, end_index):\n",
    "    indices = range(i-history_size, i, step)\n",
    "    data.append(dataset[indices])\n",
    "\n",
    "    if single_step:\n",
    "      labels.append(target[i+target_size])\n",
    "    else:\n",
    "      labels.append(target[i:i+target_size])\n",
    "\n",
    "  return np.array(data), np.array(labels)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we segment the data into 24 hour periods to train the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvms_past_history = 24\n",
    "mvms_future_target = 24\n",
    "mvms_STEP = 1\n",
    "\n",
    "x_train_multi, y_train_multi = multivariate_data(ucdUsageDataArray, ucdUsageDataArray[:, 0], 0,\n",
    "                                                 TRAIN_SPLIT, mvms_past_history,\n",
    "                                                 mvms_future_target, mvms_STEP)\n",
    "x_val_multi, y_val_multi = multivariate_data(ucdUsageDataArray, ucdUsageDataArray[:, 0],\n",
    "                                             TRAIN_SPLIT, None, mvms_past_history,\n",
    "                                             mvms_future_target, mvms_STEP)\n",
    "\n",
    "print ('Single window of past history : {}'.format(x_train_multi[0].shape))\n",
    "print ('\\n Target usage to predict : {}'.format(y_train_multi[0].shape))\n",
    "\n",
    "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we layout the model and select the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_step_model = tf.keras.models.Sequential()\n",
    "\n",
    "multi_step_model.add(tf.keras.layers.Conv1D(filters=10, kernel_size=3, activation='relu', input_shape=x_train_multi.shape[-2:]))\n",
    "multi_step_model.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "multi_step_model.add(tf.keras.layers.Dropout(rate=0.1))\n",
    "\n",
    "multi_step_model.add(tf.keras.layers.Conv1D(filters=20, kernel_size=3, activation='relu', input_shape=x_train_multi.shape[-2:]))\n",
    "multi_step_model.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "multi_step_model.add(tf.keras.layers.Dropout(rate=0.1))\n",
    "\n",
    "multi_step_model.add(tf.keras.layers.LSTM(200, activation='relu',return_sequences=True))\n",
    "multi_step_model.add(tf.keras.layers.LSTM(25, activation='relu'))\n",
    "\n",
    "multi_step_model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "\n",
    "multi_step_model.add(tf.keras.layers.Dense(24))\n",
    "\n",
    "multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae', metrics=['mean_squared_error',\n",
    "                                                                                                    root_mean_squared_error])\n",
    "\n",
    "for x, y in val_data_multi.take(1):\n",
    "  print (multi_step_model.predict(x).shape)\n",
    "\n",
    "multi_step_history = multi_step_model.fit(train_data_multi, epochs=10,\n",
    "                                          steps_per_epoch=200,\n",
    "                                          validation_data=val_data_multi,\n",
    "                                          validation_steps=50)\n",
    "\n",
    "plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make 2560 predictions using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_data = []\n",
    "rnn_comp_pred = []\n",
    "ucd_svr_comp_pred = []\n",
    "    \n",
    "for x, y in val_data_multi.take(10):\n",
    "    for a in range(len(x)):\n",
    "        rnn_data.append(y[a])\n",
    "        ucd_svr_comp_pred.append(ucd_reg_svr.predict(x[a]))\n",
    "        rnn_comp_pred.append(multi_step_model.predict(x)[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get30days(mainArray):\n",
    "    firstBool = False\n",
    "    d30a = np.array([])\n",
    "    \n",
    "    for x in range(30):\n",
    "        if firstBool == False:\n",
    "            d30a = np.concatenate((mainArray[int(24*x)], mainArray[int(24*(x+1))]), axis = 0)\n",
    "            firstBool = True\n",
    "        else:\n",
    "            d30a = np.concatenate((d30a, mainArray[int(24*(x+1))]), axis = 0)\n",
    "    \n",
    "    return d30a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_data_30 = get30days(rnn_data)\n",
    "rnn_pred_30 = get30days(rnn_comp_pred)\n",
    "svr_pred_30 = get30days(ucd_svr_comp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = np.arange(0,len(rnn_data_30[:120]))\n",
    "\n",
    "plt.plot(dates, rnn_data_30[:120], c='b', label='Data')\n",
    "plt.plot(dates, rnn_pred_30[:120], c= 'y', label='RNN model')\n",
    "plt.plot(dates, svr_pred_30[:120], c= 'r', label='SVR model')\n",
    "    \n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Prediction Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(rnn_data_30,rnn_pred_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(rnn_data_30,svr_pred_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twentyfour_hour_mae(y_pred, y_true):\n",
    "    mae = []\n",
    "    for a in range(len(y_pred)):\n",
    "        mae.append(mean_absolute_error(y_pred[a],y_true[a]))\n",
    "    \n",
    "    return (np.array(mae))\n",
    "\n",
    "def twentyfour_hour_rmse(y_pred, y_true):\n",
    "    rmse = []\n",
    "    for a in range(len(y_pred)):\n",
    "        rmse.append(simple_rmse(y_pred[a],y_true[a]))\n",
    "    \n",
    "    return (np.array(rmse))\n",
    "\n",
    "def twentyfour_hour_r_squared(y_pred, y_true):\n",
    "    r_squared = []\n",
    "    for a in range(len(y_pred)):\n",
    "        r_squared.append(get_r_squared(y_pred[a],y_true[a]))\n",
    "    \n",
    "    return (np.array(r_squared))\n",
    "\n",
    "def smape(actual, forecasted):\n",
    "    return 1/len(actual) * np.sum(2 * np.abs(forecasted - actual) / (np.abs(actual) + np.abs(forecasted)))\n",
    "\n",
    "def twentyfour_hour_smape(y_pred, y_true):\n",
    "    smape_arr = []\n",
    "    for a in range(len(y_pred)):\n",
    "        smape_arr.append(smape(y_pred[a],y_true[a]))\n",
    "    \n",
    "    return (np.array(smape_arr))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "def simple_rmse(y_true, y_pred):\n",
    "    return sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the dating using MAE, RMSE and SMAPE and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_batch_mae = (twentyfour_hour_mae(rnn_comp_pred,rnn_data))\n",
    "rnn_batch_rmse = (twentyfour_hour_rmse(rnn_comp_pred,rnn_data))\n",
    "rnn_batch_smape = (twentyfour_hour_smape(rnn_data,rnn_comp_pred))\n",
    "\n",
    "svm_batch_mae = (twentyfour_hour_mae(ucd_svm_comp_pred,rnn_data))\n",
    "svm_batch_rmse = (twentyfour_hour_rmse(ucd_svm_comp_pred,rnn_data))\n",
    "svm_batch_smape = (twentyfour_hour_smape(rnn_data, ucd_svm_comp_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = np.arange(0,2560)\n",
    "\n",
    "plt.plot(dates, rnn_batch_mae.cumsum(), color = 'b', label='RNN Error')\n",
    "plt.plot(dates, svm_batch_mae.cumsum(), color = 'g', label='SVM Error')\n",
    "\n",
    "plt.xlabel('24 Hour Periods')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Error Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = np.arange(0,2560)\n",
    "\n",
    "plt.plot(dates, rnn_batch_rmse.cumsum(), color = 'b', label='RNN Error')\n",
    "plt.plot(dates, svm_batch_rmse.cumsum(), color = 'g', label='SVM Error')\n",
    "\n",
    "plt.xlabel('24 Hour Periods')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Error Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = np.arange(0,2560)\n",
    "\n",
    "plt.plot(dates, rnn_batch_smape.cumsum(), color = 'b', label='RNN Error')\n",
    "plt.plot(dates, svm_batch_smape.cumsum(), color = 'g', label='SVM Error')\n",
    "\n",
    "plt.xlabel('24 Hour Periods')\n",
    "plt.ylabel('SMAPE')\n",
    "plt.title('Error Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmBars = [np.mean(svm_batch_mae), np.mean(svm_batch_rmse)]\n",
    "rnnBars = [np.mean(rnn_batch_mae), np.mean(rnn_batch_rmse)]\n",
    "\n",
    "groupedBarErrorData = np.array([[\"SVM\", \"MAE\", np.mean(svm_batch_mae)],\n",
    "                                [\"SVM\",\"RMSE\", np.mean(svm_batch_rmse)], \n",
    "                                [\"SVM\", \"SMAPE\", np.mean(svm_batch_smape)], \n",
    "                                [\"RNN\", \"MAE\", np.mean(rnn_batch_mae)],\n",
    "                                [\"RNN\", \"RMSE\", np.mean(rnn_batch_rmse)],\n",
    "                                [\"RNN\", \"SMAPE\", np.mean(rnn_batch_smape)]\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedBarErrorDataFrame = pd.DataFrame(groupedBarErrorData, columns=[\"Model\", \"Metric\", \"Accuracy\"])\n",
    "\n",
    "groupedBarErrorDataFrame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(x=\"Metric\", y=\"Accuracy\", hue=\"Model\", data=groupedBarErrorDataFrame,\n",
    "                height=6, kind=\"bar\", palette=\"bright\")\n",
    "g.despine(left=True)\n",
    "g.set_ylabels(\"Error\\n\", fontsize = 16)\n",
    "g.set_xlabels(\"\\nMetric\", fontsize = 16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
